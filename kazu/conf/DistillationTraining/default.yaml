seed: 42
cudnn:
  deterministic: True
  benchmark: False
monitoring:
  monitor: entity_f1
  mode: max
training_params:
  max_epochs: 10
save_dir: "." # save dir managed by hydra. Override here if required
model:
  _target_: kazu.distillation.models.SequenceTaggingDistillationForFinalLayer
  data_dir: ???
  label_list:
    - B-disease
    - B-drug
    - B-gene
    - B-mutation
    - B-species
    - I-disease
    - I-drug
    - I-gene
    - I-mutation
    - I-species
    - O
  student_model_path: ???
  teacher_model_path: ???
  batch_size: 8
  max_length: 128
  num_workers: 2
  temperature: 1.0
  warmup_steps: 0
  learning_rate: 5e-5
  schedule: warmup_linear
  weight_decay: 0.01
  accumulate_grad_batches: ${DistillationTraining.trainer.accumulate_grad_batches}
  max_epochs: ${DistillationTraining.training_params.max_epochs}
  metric: ${DistillationTraining.monitoring.monitor}
trainer:
  _convert_: 'partial' #needed to convert ListConfig to list for Pytorch lightning plugins parameter, which needs a list (ver 1.6.4)
  _target_: pytorch_lightning.Trainer
  num_sanity_val_steps: 2
  accelerator: "cpu"
  val_check_interval: 1.0
  accumulate_grad_batches: 1
  max_epochs: ${DistillationTraining.training_params.max_epochs}
  logger:
    - _target_: pytorch_lightning.loggers.CSVLogger
      save_dir: ${DistillationTraining.save_dir}
      name: csv_log
  plugins:
    - _target_: kazu.distillation.lightning_plugins.StudentModelCheckpointIO
      model_name_or_path: ${DistillationTraining.model.student_model_path}
  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: ${DistillationTraining.save_dir}
      filename: "student_model-{epoch:02d}-{entity_f1:.4f}-{validation_loss:.3f}-{step:05d}"
      monitor: ${DistillationTraining.monitoring.monitor}
      mode: ${DistillationTraining.monitoring.mode}
      save_top_k: 5
      save_last: True
      every_n_train_steps: ~
      every_n_epochs: ~
    - _target_: pytorch_lightning.callbacks.early_stopping.EarlyStopping
      monitor: ${DistillationTraining.monitoring.monitor}
      mode: ${DistillationTraining.monitoring.mode}
      min_delta: 0.00
      patience: 5
      verbose: True
    - _target_: pytorch_lightning.callbacks.progress.TQDMProgressBar
      refresh_rate: 1

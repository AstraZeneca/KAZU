import logging
import traceback
from typing import List, Tuple, Dict, Optional

import pydash
import torch
from pytorch_lightning import Trainer
from torch import Tensor
from torch.nn import Softmax
from torch.utils.data import DataLoader
from transformers import (
    AutoModelForTokenClassification,
    AutoConfig,
    AutoTokenizer,
    DataCollatorWithPadding,
    BatchEncoding,
)
from transformers.file_utils import PaddingStrategy

from kazu.data.data import Document, Section, PROCESSING_EXCEPTION, TokenizedWord
from kazu.data.pytorch import HFDataset
from kazu.modelling.hf_lightning_wrappers import PLAutoModelForTokenClassification
from kazu.steps import BaseStep
from kazu.steps.ner.bio_label_preprocessor import TokenizedWordProcessor
from kazu.steps.ner.entity_post_processing import NonContiguousEntitySplitter
from kazu.utils.utils import documents_to_document_section_batch_encodings_map

logger = logging.getLogger(__name__)


class TransformersModelForTokenClassificationNerStep(BaseStep):
    """
    An wrapper for :class:`transformers.AutoModelForTokenClassification' and
    :class:`kazu.steps.ner.bio_label_preprocessor.BioLabelPreProcessor`. This implementation uses a sliding
    window concept to process large documents that don't fit into the maximum sequence length allowed by a model.

    """

    def __init__(
        self,
        path: str,
        depends_on: List[str],
        batch_size: int,
        stride: int,
        max_sequence_length: int,
        trainer: Trainer,
        entity_splitter: Optional[NonContiguousEntitySplitter] = None,
    ):
        """
        :param stride: passed to HF tokenizers (for splitting long docs)
        :param max_sequence_length: passed to HF tokenizers (for splitting long docs)
        :param path: path to HF model, config and tokenizer. Passed to HF .from_pretrained()
        :param depends_on:
        :param batch_size: batch size for dataloader
        :param debug: print extra logging info
        """
        super().__init__(depends_on=depends_on)
        self.entity_splitter = entity_splitter
        if max_sequence_length % 2 != 0:
            raise RuntimeError(
                "max_sequence_length must %2 ==0 in order for correct document windowing"
            )
        self.max_sequence_length = max_sequence_length
        if stride % 2 != 0:
            raise RuntimeError("stride must %2 ==0 in order for correct document windowing")
        self.stride = stride
        self.batch_size = batch_size
        self.config = AutoConfig.from_pretrained(path)
        self.tokeniser = AutoTokenizer.from_pretrained(path, config=self.config)
        self.model = AutoModelForTokenClassification.from_pretrained(path, config=self.config)
        self.model = PLAutoModelForTokenClassification(self.model).eval()
        self.trainer = trainer
        self.softmax = Softmax(dim=-1)
        self.bio_preprocessor = TokenizedWordProcessor(confidence_threshold=0.03)

    def get_dataloader(self, docs: List[Document]) -> Tuple[DataLoader, Dict[int, Section]]:
        """
        get a dataloader from a List of Document. Collation is handled via DataCollatorWithPadding

        :param docs:
        :return: a tuple of dataloader, and a dict of int:Section. The int maps to overflow_to_sample_mapping in the
                underlying batch encoding, allowing the processing of docs longer than can fit within the maximum
                sequence length of a transformer
        """
        batch_encoding, id_section_map = documents_to_document_section_batch_encodings_map(
            docs, self.tokeniser, stride=self.stride, max_length=self.max_sequence_length
        )
        dataset = HFDataset(batch_encoding)
        collate_func = DataCollatorWithPadding(
            tokenizer=self.tokeniser, padding=PaddingStrategy.MAX_LENGTH
        )
        loader = DataLoader(dataset=dataset, batch_size=self.batch_size, collate_fn=collate_func)
        return loader, id_section_map

    def frame_to_tok_word(
        self,
        batch_encoding: BatchEncoding,
        number_of_frames: int,
        frame_index: int,
        section_frame_index: int,
        predictions: Tensor,
    ) -> List[TokenizedWord]:
        """
        depending on the number of frames generated by a string of text, and whether it is the first or last frame,
        we need to return different subsets of the frame offsets and frame word_ids
        :param batch_encoding: a HF BatchEncoding
        :param number_of_frames: number of frames created by the tokenizer for the string
        :param frame_index: the index of the query frame, relative to the total number of frames
        :param section_frame_index: the index of the section frame, relative to the whole BatchEncoding
        :return: Tuple of 2 lists: frame offsets and frame word ids
        """
        half_stride = int(self.stride / 2)
        # 1:-1 skip cls and sep tokens
        if number_of_frames == 1:
            start_index = 1
            end_index = -1
        elif number_of_frames > 1 and frame_index == 0:
            start_index = 1
            end_index = -(half_stride + 1)
        elif number_of_frames > 1 and frame_index == number_of_frames - 1:
            start_index = half_stride + 1
            end_index = -1
        else:
            start_index = half_stride + 1
            end_index = -(half_stride + 1)

        frame_offsets = batch_encoding.encodings[section_frame_index].offsets[start_index:end_index]
        frame_word_ids = batch_encoding.encodings[section_frame_index].word_ids[
            start_index:end_index
        ]
        frame_input_ids = batch_encoding.encodings[section_frame_index].ids[start_index:end_index]
        frame_tokens = batch_encoding.encodings[section_frame_index].tokens[start_index:end_index]
        predictions = predictions[section_frame_index][start_index:end_index]

        prev_word_id = 0
        word = TokenizedWord()
        word.id2label = self.config.id2label
        all_words = []
        for i, word_id in enumerate(frame_word_ids):
            if word_id is not None:
                if word_id != prev_word_id:
                    word.decoded = self.tokeniser.decode(word.token_ids)
                    # new word
                    all_words.append(word)
                    word = TokenizedWord()
                    word.id2label = self.config.id2label
                word.token_ids.append(frame_input_ids[i])
                word.word_offsets.append(frame_offsets[i])
                word.word_confidences.append(predictions[i])
                word.tokens.append(frame_tokens[i])
                prev_word_id = word_id

        word.decoded = self.tokeniser.decode(word.token_ids)
        all_words.append(word)

        logger.debug(
            f"inputs this frame: {self.tokeniser.decode(frame_input_ids[:start_index])}<-IGNORED "
            f"START->{self.tokeniser.decode(frame_input_ids[start_index:end_index])}"
            f"<-END IGNORED->{self.tokeniser.decode(frame_input_ids[end_index:])}"
        )
        return all_words

    def section_frames_to_tokenised_words(
        self,
        section_index: int,
        batch_encoding: BatchEncoding,
        predictions: Tensor,
    ) -> List[TokenizedWord]:
        words = []

        section_frame_indices = self.get_list_of_batch_encoding_frames_for_section(
            batch_encoding, section_index
        )
        for frame_index, section_frame_index in enumerate(section_frame_indices):
            word_sub_list = self.frame_to_tok_word(
                batch_encoding=batch_encoding,
                number_of_frames=len(section_frame_indices),
                frame_index=frame_index,
                section_frame_index=section_frame_index,
                predictions=predictions,
            )
            words.extend(word_sub_list)
        return words

    def _run(self, docs: List[Document]) -> Tuple[List[Document], List[Document]]:
        failed_docs = []
        try:
            loader, id_section_map = self.get_dataloader(docs)
            # run the transformer and get results
            softmax = self.get_softmax_predictions(loader)
            for section_index, section in id_section_map.items():
                words = self.section_frames_to_tokenised_words(
                    section_index=section_index,
                    batch_encoding=loader.dataset.encodings,
                    predictions=softmax,
                )
                entities = self.bio_preprocessor(
                    words, text=section.get_text(), namespace=self.namespace()
                )
                section.entities.extend(entities)
                if self.entity_splitter:
                    split_ents = pydash.flatten(
                        [self.entity_splitter(x, section.get_text()) for x in entities]
                    )
                    section.entities.extend(split_ents)
        except Exception:
            affected_doc_ids = [doc.idx for doc in docs]
            for doc in docs:
                message = (
                    f"batch failed: affected ids: {affected_doc_ids}\n" + traceback.format_exc()
                )
                doc.metadata[PROCESSING_EXCEPTION] = message
                failed_docs.append(doc)

        return docs, failed_docs

    def get_list_of_batch_encoding_frames_for_section(
        self, batch_encoding: BatchEncoding, section_index: int
    ) -> List[int]:
        """
        for a given dataloader with a HFDataset, return a list of frame indexes associated with a given section index
        :param loader:
        :param section_index:
        :return:
        """
        section_frame_indices = [
            i
            for i, x in enumerate(batch_encoding.data["overflow_to_sample_mapping"])
            if x == section_index
        ]
        return section_frame_indices

    def get_softmax_predictions(self, loader: DataLoader) -> Tensor:
        """
        get a namedtuple_values_indices consisting of confidence and labels for a given dataloader (i.e. run bert)
        :param loader:
        :return:
        """
        results = torch.cat(
            [
                x.logits
                for x in self.trainer.predict(
                    model=self.model, dataloaders=loader, return_predictions=True
                )
            ]
        )
        softmax = self.softmax(results)
        # get confidence scores and label ints
        # confidence_and_labels_tensor = torch.max(softmax, dim=-1)
        return softmax
